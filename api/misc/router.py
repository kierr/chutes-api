"""
Router for misc. stuff, e.g. score proxy.
"""

import uuid
import asyncio
import aiohttp
import orjson as json
from loguru import logger
from typing import Optional
from huggingface_hub import HfApi
from huggingface_hub.utils import (
    RepositoryNotFoundError,
    RevisionNotFoundError,
    GatedRepoError,
    HfHubHTTPError,
)
from urllib.parse import urlparse
from fastapi import APIRouter, Request, Response, HTTPException, status, Query
from fastapi.responses import StreamingResponse
from typing import AsyncIterator
from api.config import settings
from api.chute.util import get_one

router = APIRouter()

ALLOWED_DOMAINS = [
    "scoredata.me",
]


def is_url_allowed(url: str) -> bool:
    parsed = urlparse(url)
    return any(domain in parsed.netloc for domain in ALLOWED_DOMAINS)


@router.get("/proxy")
async def proxy(
    url: str,
    request: Request,
    stream: bool = Query(False, description="Stream the response for large files/videos"),
):
    if url == "ping":
        return {"pong": True}

    if not url.startswith(("http://", "https://")) or not is_url_allowed(url):
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Invalid or unauthorized URL.",
        )

    # Configure headers to forward.
    headers_to_forward = {}
    skip_headers = {
        "host",
        "connection",
        "content-length",
        "content-encoding",
        "transfer-encoding",
        "upgrade",
    }
    for header_name, header_value in request.headers.items():
        if header_name.lower() not in skip_headers:
            headers_to_forward[header_name] = header_value
    timeout = aiohttp.ClientTimeout(connect=10.0, total=300.0)

    # Headers to forward from the upstream response
    forward_response_headers = [
        "content-type",
        "content-length",
        "accept-ranges",
        "content-range",
        "cache-control",
        "etag",
        "last-modified",
        "expires",
        "date",
    ]

    try:
        if stream:
            session = aiohttp.ClientSession(timeout=timeout)
            response = await session.get(url, headers=headers_to_forward)
            if not response.ok:
                await response.close()
                await session.close()
                raise HTTPException(
                    status_code=response.status,
                    detail=f"Upstream server returned {response.status}",
                )
            response_headers = {}
            for header in forward_response_headers:
                if header in response.headers:
                    response_headers[header] = response.headers[header]

            async def stream_content() -> AsyncIterator[bytes]:
                try:
                    async for chunk in response.content.iter_chunked(8192):
                        yield chunk
                finally:
                    await response.close()
                    await session.close()

            return StreamingResponse(
                stream_content(),
                status_code=response.status,
                headers=response_headers,
                media_type=response_headers.get("content-type", "application/octet-stream"),
            )

        else:
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(url, headers=headers_to_forward) as response:
                    content = await response.read()

                    response_headers = {}
                    for header in forward_response_headers:
                        if header in response.headers:
                            response_headers[header] = response.headers[header]

                    return Response(
                        content=content, status_code=response.status, headers=response_headers
                    )

    except HTTPException:
        raise
    except aiohttp.ClientTimeout:
        logger.error(f"WHITELIST_PROXY: upstream gateway timeout: {url=} {stream=}")
        raise HTTPException(
            status_code=status.HTTP_504_GATEWAY_TIMEOUT, detail="Upstream server timeout"
        )
    except aiohttp.ClientError as e:
        logger.error(
            f"WHITELIST_PROXY: upstream gateway request failed: {url=} {stream=} exception={str(e)}"
        )
        raise HTTPException(
            status_code=status.HTTP_502_BAD_GATEWAY,
            detail=f"Upstream server returned error: {str(e)}",
        )
    except Exception as e:
        logger.error(
            f"WHITELIST_PROXY: unhandled exception proxying upstream request: {url=} {stream=} exception={str(e)}"
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Unhandled exception proxying request: {str(e)}",
        )


def _fetch_repo_info_sync(repo_id: str, repo_type: str, revision: str, hf_token: Optional[str]):
    """
    Load huggingface repo info for cache validation.
    """
    api = HfApi(token=hf_token)
    repo_items = api.list_repo_tree(
        repo_id=repo_id,
        revision=revision,
        repo_type=repo_type,
        recursive=True,
    )
    files = []
    directories = []
    for item in repo_items:
        # Directories don't have size, but have tree_id
        if not hasattr(item, "size"):
            directories.append(item.path)
            continue
        file_info = {
            "path": item.path,
            "size": getattr(item, "size", None),
        }
        if hasattr(item, "lfs") and item.lfs:
            file_info["sha256"] = item.lfs.sha256
            file_info["is_lfs"] = True
        else:
            file_info["blob_id"] = getattr(item, "blob_id", None)
            file_info["is_lfs"] = False
        files.append(file_info)

    return {
        "repo_id": repo_id,
        "repo_type": repo_type,
        "revision": revision,
        "files": files,
        "directories": directories,
    }


@router.get("/hf_repo_info")
async def get_hf_repo_info(
    repo_id: str = Query(...),
    repo_type: str = Query("model"),
    revision: str = Query("main"),
    hf_token: Optional[str] = Query(None),
):
    """
    Proxy endpoint for HF repo file info.
    """
    uid = str(uuid.uuid5(uuid.NAMESPACE_OID, f"{repo_id=},{repo_type=},{revision=},{hf_token=}"))
    cache_key = f"hf_repo_info:{uid}"
    cached = await settings.redis_client.get(cache_key)
    if cached:
        try:
            return json.loads(cached)
        except Exception:
            await settings.redis_client.delete(cache_key)

    # Chute exists?
    chute = await get_one(repo_id)
    if not chute:
        chute = await get_one(f"{repo_id}-TEE")
    if not chute:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail=f"No chute found for model {repo_id}"
        )

    try:
        result = await asyncio.to_thread(
            _fetch_repo_info_sync, repo_id, repo_type, revision, hf_token
        )
    except RepositoryNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Repository '{repo_id}' not found on HuggingFace",
        )
    except RevisionNotFoundError:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Revision '{revision}' not found in repository '{repo_id}'",
        )
    except GatedRepoError:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail=f"Repository '{repo_id}' is gated; valid HF token required",
        )
    except HfHubHTTPError as e:
        if e.response is not None and e.response.status_code in (401, 403):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Access denied to '{repo_id}': invalid or missing credentials",
            )
        raise HTTPException(status_code=status.HTTP_502_BAD_GATEWAY, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e))

    await settings.redis_client.set(cache_key, json.dumps(result).decode())
    return result
