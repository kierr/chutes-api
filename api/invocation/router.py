"""
Invocations router.
"""

import re
import pybase64 as base64
import pickle
import asyncio
import gzip
import orjson as json
import csv
import uuid
import time
import decimal
import traceback
from loguru import logger
from pydantic import BaseModel, ValidationError, Field
from datetime import date, datetime, timedelta, UTC
from io import BytesIO, StringIO
from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, status, Request, Response
from starlette.responses import StreamingResponse
from sqlalchemy import text, select
from sqlalchemy.ext.asyncio import AsyncSession
from api.config import settings
from api.chute.util import (
    invoke,
    get_one,
    get_llms,
    is_shared,
    count_prompt_tokens,
)
from api.util import (
    recreate_vlm_payload,
    has_legacy_private_billing,
)
from api.user.schemas import User, InvocationQuota
from api.user.service import get_current_user, chutes_user_id, subnet_role_accessible
from api.report.schemas import Report, ReportArgs
from api.database import get_db_session, get_session, get_inv_session, get_db_ro_session
from api.instance.util import get_chute_target_manager
from api.invocation.util import get_prompt_prefix_hashes, get_sponsored_chute_ids
from api.util import validate_tool_call_arguments
from api.permissions import Permissioning

router = APIRouter()
host_invocation_router = APIRouter()


class DiffusionInput(BaseModel):
    prompt: str
    negative_prompt: str = ""
    height: int = Field(default=1024, ge=128, le=2048)
    width: int = Field(default=1024, ge=128, le=2048)
    num_inference_steps: int = Field(default=25, ge=1, le=50)
    guidance_scale: float = Field(default=7.5, ge=1.0, le=20.0)
    seed: Optional[int] = Field(default=None, ge=0, le=2**32 - 1)
    img_guidance_scale: float = Field(default=7.5, ge=1.0, le=20.0)
    image_b64: Optional[list[str]] = Field(
        default=None, description="Base64 encoded images for image-to-image pipelines."
    )
    true_cfg_scale: Optional[float] = Field(default=None, ge=0.0, le=10.0)

    class Config:
        extra = "forbid"


async def initialize_quota_cache(cache_key: str) -> None:
    await settings.redis_client.incrbyfloat(cache_key, 0.0)


@router.get("/usage")
async def get_usage(request: Request):
    """
    Get aggregated usage data, which is the amount of revenue
    we would be receiving if no usage was free.
    """
    cache_key = "invocation_usage_data"
    if request:
        if (cached := await settings.redis_client.get(cache_key)) is not None:
            return json.loads(cached)
    query = text(
        "SELECT chute_id, DATE(bucket) as date, sum(amount) as usd_amount, sum(count) as invocation_count "
        "from usage_data "
        "where bucket >= now() - interval '11 days' "
        "group by chute_id, date "
        "order by date desc, usd_amount desc"
    )
    async with get_session() as session:
        result = await session.execute(query)
        rv = []
        for chute_id, date, usd_amount, invocation_count in result:
            rv.append(
                {
                    "chute_id": chute_id,
                    "date": date,
                    "usd_amount": float(usd_amount),
                    "invocation_count": int(invocation_count),
                }
            )
        await settings.redis_client.set(cache_key, json.dumps(rv))
        return rv


async def _cached_get_metrics(table, cache_key):
    if (cached := await settings.redis_client.get(cache_key)) is not None:
        return json.loads(gzip.decompress(base64.b64decode(cached)))
    async with get_inv_session() as session:
        result = await session.execute(text(f"SELECT * FROM {table}"))
        rows = result.mappings().all()
        rv = [dict(row) for row in rows]
        for row in rv:
            for key, value in row.items():
                if isinstance(value, decimal.Decimal):
                    row[key] = float(value)
        cache_value = base64.b64encode(gzip.compress(json.dumps(rv)))
        await settings.redis_client.set(cache_key, cache_value, ex=300)
        return rv


@router.get("/stats/llm")
async def get_llm_stats():
    return await _cached_get_metrics("vllm_metrics", b"llmstats")


@router.get("/stats/diffusion")
async def get_diffusion_stats():
    return await _cached_get_metrics("diffusion_metrics", b"diffstats")


@router.get("/exports/{year}/{month}/{day}/{hour_format}")
async def get_export(
    year: int,
    month: int,
    day: int,
    hour_format: str,
) -> Response:
    """
    Get invocation exports (and reports) for a particular hour.
    """
    format_match = re.match(r"^(\d+)((?:-(reports|jobs))?\.csv)$", hour_format)
    if not format_match:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST, detail=f"Invalid format: {hour_format}"
        )
    hour = int(format_match.group(1))
    suffix = format_match.group(2)

    # Sanity check the dates.
    valid = True
    if (
        (not 2024 <= year <= date.today().year)
        or not (1 <= month <= 12)
        or not (1 <= day <= 31)
        or not (0 <= hour <= 23)
    ):
        valid = False
    target_date = datetime(year, month, day, hour)
    today = date.today()
    current_hour = datetime.utcnow()
    if (
        target_date > datetime.utcnow()
        or target_date < datetime(2024, 12, 14, 0)
        or (target_date.date == today and hour == current_hour)
    ):
        valid = False
    if not valid:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Invocations export not found {year=} {month=} {day=} {hour=}",
        )

    # Construct the S3 key based on whether this is a reports request
    key = f"invocations/{year}/{month:02d}/{day:02d}/{hour:02d}{suffix}"

    # Check if the file exists
    exists = False
    async with settings.s3_client() as s3:
        try:
            await s3.head_object(Bucket=settings.storage_bucket, Key=key)
            exists = True
        except Exception as exc:
            if exc.response["Error"]["Code"] != "404":
                raise

    if not exists:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Invocations export not found {year=} {month=} {day=} {hour=}",
        )

    # Download and return the file.
    data = BytesIO()
    async with settings.s3_client() as s3:
        await s3.download_fileobj(settings.storage_bucket, key, data)
    filename = key.replace("invocations/", "").replace("/", "-")
    return Response(
        content=data.getvalue(),
        media_type="text/csv",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


@router.get("/exports/recent")
async def get_recent_export(
    hotkey: Optional[str] = None,
    limit: Optional[int] = 100,
    db: AsyncSession = Depends(get_db_ro_session),
):
    """
    Get an export for recent data, which may not yet be in S3.
    """
    query = """
        SELECT
            invocation_id,
            chute_id,
            chute_user_id,
            function_name,
            image_id,
            image_user_id,
            instance_id,
            miner_uid,
            miner_hotkey,
            started_at,
            completed_at,
            error_message,
            compute_multiplier,
            metrics
        FROM partitioned_invocations
        WHERE started_at >= CURRENT_TIMESTAMP - INTERVAL '1 day'
        AND completed_at IS NOT NULL
        AND error_message IS NULL
    """
    if not limit or limit <= 0:
        limit = 100
    limit = min(limit, 10000)
    params = {"limit": limit}
    if hotkey:
        query += " AND miner_hotkey = :hotkey"
        params["hotkey"] = hotkey
    query += " ORDER BY started_at DESC LIMIT :limit"
    output = StringIO()
    writer = csv.writer(output)
    result = await db.execute(text(query), params)
    writer.writerow([col for col in result.keys()])
    rows = result.fetchall()
    await asyncio.to_thread(writer.writerows, rows)
    return Response(
        content=output.getvalue(),
        media_type="text/csv",
        headers={"Content-Disposition": 'attachment; filename="recent.csv"'},
    )


@router.post("/{invocation_id}/report")
async def report_invocation(
    invocation_id: str,
    report_args: ReportArgs,
    db: AsyncSession = Depends(get_db_session),
    current_user: User = Depends(get_current_user()),
):
    # Make sure the invocation exists and there isn't already a report.
    report_exists = (
        await db.execute(
            select(
                text(
                    "EXISTS (SELECT 1 FROM reports WHERE invocation_id = :invocation_id)"
                ).bindparams(invocation_id=invocation_id)
            )
        )
    ).scalar()
    if report_exists:
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail="A report has already been filed for this invocation",
        )

    report = Report(
        invocation_id=invocation_id,
        user_id=current_user.user_id,
        reason=report_args.reason,
    )
    db.add(report)
    await db.commit()
    return {
        "status": f"report received for {invocation_id=}",
    }


async def _invoke(
    request: Request,
    current_user: User,
):
    # Check if the user has access.
    chute = await get_one(request.state.chute_id)
    if not chute:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail="No matching chute found!"
        )
    if not (
        chute.public
        or chute.user_id == current_user.user_id
        or await is_shared(chute.chute_id, current_user.user_id)
        or subnet_role_accessible(chute, current_user)
    ):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND, detail="No matching chute found!"
        )

    # Check if the chute is disabled.
    if chute.disabled:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="This chute is currently disabled.",
        )

    # Check X-TEE-Only header - if set to true, require TEE-enabled chute
    tee_only_header = request.headers.get("X-TEE-Only", "").lower()
    if tee_only_header == "true" and not chute.tee:
        raise HTTPException(
            status_code=status.HTTP_426_UPGRADE_REQUIRED,
            detail="This chute does not have TEE enabled. Use the /teeify endpoint to promote the chute to TEE, or remove the X-TEE-Only header.",
        )

    quota_date = date.today()
    if chute.discount == 1.0:
        request.state.free_invocation = True

        # Limit free model usage independently of quota.
        if current_user.permissions_bitmask == 0:
            effective_balance = (
                current_user.current_balance.effective_balance
                if current_user.current_balance
                else 0.0
            )
            unlimited = False
            if effective_balance >= 10:
                unlimited = True
            else:
                quota = await InvocationQuota.get(current_user.user_id, "__anychute__")
                if quota > 2000:
                    unlimited = True
            if not unlimited:
                free_usage = 0
                try:
                    qkey = f"free_usage:{quota_date}:{current_user.user_id}"
                    free_usage = await settings.redis_client.incr(qkey)
                    if free_usage <= 3:
                        tomorrow = datetime.combine(quota_date, datetime.min.time()) + timedelta(
                            days=1
                        )
                        exp = max(int((tomorrow - datetime.now()).total_seconds()), 1)  # noqa
                except Exception as exc:
                    logger.warning(
                        f"Error checking free usage for {current_user.user_id=}: {str(exc)}"
                    )
                if free_usage > 100:
                    logger.warning(
                        f"{current_user.user_id=} {current_user.username=} has hit daily free limit: {chute.name=} {effective_balance=}"
                    )
                    raise HTTPException(
                        status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                        detail="Free models limit reached for today - maintain >= $10 balance or upgrade subscription to pro to unlock more.",
                    )
    elif current_user.user_id == settings.or_free_user_id:
        sponsored_chutes = await get_sponsored_chute_ids(current_user.user_id)
        if chute.chute_id not in sponsored_chutes:
            logger.warning(
                f"Attempt to invoke {chute.chute_id=} {chute.name=} from openrouter free account."
            )
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Invalid free model, please select from the updated list of current chutes free models",
            )

    # Check account balance.
    origin_ip = request.headers.get("x-forwarded-for", "").split(",")[0]

    # Prevent calling private chutes when the owner has no balance.
    if (
        not chute.public
        and not has_legacy_private_billing(chute)
        and chute.user_id != await chutes_user_id()
    ):
        owner_balance = (
            chute.user.current_balance.effective_balance if chute.user.current_balance else 0.0
        )
        if owner_balance <= 0:
            logger.warning(
                f"Preventing execution of chute {chute.name=} {chute.chute_id=}, "
                f"creator has insufficient balance {owner_balance=}"
            )
            raise HTTPException(
                status_code=status.HTTP_402_PAYMENT_REQUIRED,
                detail="Chute unavailable because the creator of this chute {chute.user_id=} has zero balance.",
            )
        request.state.free_invocation = True

    # Check account quotas if not free/invoiced.
    quota_date = date.today()
    if not (
        current_user.has_role(Permissioning.free_account)
        or current_user.has_role(Permissioning.invoice_billing)
        or request.state.free_invocation
    ):
        quota = await InvocationQuota.get(current_user.user_id, chute.chute_id)
        key = await InvocationQuota.quota_key(current_user.user_id, chute.chute_id)
        client_success, cached = await settings.redis_client.get_with_status(key)
        request_count = 0.0
        if cached is not None:
            try:
                request_count = float(cached.decode())
            except ValueError:
                await settings.redis_client.delete(key)
        elif client_success:
            asyncio.create_task(initialize_quota_cache(key))

        # No quota for private/user-created chutes.
        effective_balance = (
            current_user.current_balance.effective_balance if current_user.current_balance else 0.0
        )
        if (
            not chute.public
            and not has_legacy_private_billing(chute)
            and chute.user_id != await chutes_user_id()
        ):
            quota = 0

        # Automatically switch to paygo when the quota is exceeded.
        if request_count >= quota:
            if effective_balance <= 0 and not request.state.free_invocation:
                logger.warning(
                    f"Payment required: attempted invocation of {chute.name} "
                    f"from user {current_user.username} [{origin_ip}] with no balance "
                    f"and {request_count=} of {quota=}"
                )
                error_kwargs = {
                    "status_code": status.HTTP_402_PAYMENT_REQUIRED,
                    "detail": {
                        "message": (
                            f"Quota exceeded and account balance is ${current_user.current_balance.effective_balance}, "
                            f"please pay with fiat or send tao to {current_user.payment_address}"
                        ),
                    },
                }
                if quota:
                    quota_reset = quota_date + timedelta(days=1)
                    quota_reset = quota_reset = datetime(
                        year=quota_reset.year,
                        month=quota_reset.month,
                        day=quota_reset.day,
                        tzinfo=UTC,
                    ).isoformat()
                    error_kwargs["detail"]["quota_reset_timestamp"] = quota_reset

                raise HTTPException(**error_kwargs)
        else:
            # When within the quota, mark the invocation as "free" so no balance is deducted when finished.
            request.state.free_invocation = True

    # Identify the cord that we'll trying to access by the public API path and method.
    selected_cord = None
    request_body = None
    try:
        request_body = await request.json() if request.method in ("POST", "PUT", "PATCH") else {}
    except (TypeError, ValueError) as exc:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"invalid request JSON payload: {str(exc)}",
        )

    request_params = request.query_params._dict if request.query_params else {}
    stream = request_body.get("stream", request_params.get("stream", False))
    for cord in chute.cords:
        public_path = cord.get("public_api_path", None)
        if public_path and public_path == request.url.path:
            if cord.get("public_api_method", "POST") == request.method:
                if chute.standard_template != "vllm" or stream == cord.get("stream"):
                    selected_cord = cord
                    if cord.get("stream"):
                        stream = True
                    break
    if not selected_cord:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="No matching cord found!")

    # Wrap up the args/kwargs in the way the miner execution service expects them.
    args, kwargs = None, None
    prefix_hashes = None
    if chute.standard_template == "diffusion":
        request_body.pop("cord", None)
        request_body.pop("method", None)
        request_body.pop("model", None)
        steps = request_body.get("num_inference_steps")
        max_steps = 30 if chute.name == "FLUX-1.dev" else 50
        if steps and (isinstance(steps, int) or steps.isdigit()) and int(steps) > max_steps:
            request_body["num_inference_steps"] = int(max_steps)
        try:
            _ = DiffusionInput(**request_body)
        except ValidationError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST, detail="bad request, naughty naughty"
            )
    elif chute.standard_template == "vllm":
        # Force usage metrics.
        if request_body.get("stream"):
            if "stream_options" not in request_body:
                request_body["stream_options"] = {}
            if not request_body["stream_options"].get("include_usage"):
                request_body["stream_options"]["include_usage"] = True
        if request_body.get("logprobs"):
            if not request_body.get("top_logprobs"):
                request_body["top_logprobs"] = 1

        # Custom temp for Dolphin.
        if chute.name in (
            "cognitivecomputations/Dolphin3.0-R1-Mistral-24B",
            "cognitivecomputations/Dolphin3.0-Mistral-24B",
        ):
            if "temperature" not in request_body:
                request_body["temperature"] = 0.05

        # Make sure the model name is correct.
        if (requested_model := request_body.get("model")) != chute.name:
            logger.warning(
                f"User requested model {requested_model} but chute name is: {chute.name}"
            )
            request_body["model"] = chute.name

        # Fetch images/videos and convert to base64 to avoid issues with miner network policies/connectivity.
        try:
            await recreate_vlm_payload(request_body)
        except Exception as exc:
            if isinstance(exc, HTTPException):
                raise
            logger.error(f"Failed to update VLM request payload: {str(exc)}")

        # Validate tool call arguments JSON.
        try:
            await validate_tool_call_arguments(request_body)
        except Exception as exc:
            if isinstance(exc, HTTPException):
                raise
            logger.error(f"Failed to validate tool call arguments: {str(exc)}")

        # Fix LongCat thinking chat template...
        if chute.name == "meituan-longcat/LongCat-Flash-Thinking-FP8":
            try:
                for message in request_body.get("messages", []):
                    if message.get("role") == "tool":
                        if "name" not in message:
                            message["name"] = message.get("tool_call_id", "__unknown__")
            except Exception as exc:
                logger.warning(f"Failed to fix longcat flash thinking tool calls: {str(exc)}")

        # Load prompt prefixes so we can do more intelligent routing.
        prefix_hashes = get_prompt_prefix_hashes(request_body)

    is_passthrough = chute.standard_template in ("vllm", "tei") or selected_cord.get(
        "passthrough", False
    )
    if is_passthrough:
        raw_payload = {"json": request_body, "params": request_params}
    else:
        raw_payload = request_body

    # Keep pickle for < 0.5.5 backwards compat
    if is_passthrough:
        request_body = {"json": request_body, "params": request_params}
        args = base64.b64encode(gzip.compress(pickle.dumps(tuple()))).decode()
        kwargs = base64.b64encode(gzip.compress(pickle.dumps(request_body))).decode()
    else:
        args = base64.b64encode(gzip.compress(pickle.dumps((request_body,)))).decode()
        kwargs = base64.b64encode(gzip.compress(pickle.dumps({}))).decode()
    manager = await get_chute_target_manager(chute, max_wait=0)
    if not manager or not manager.instances:
        chute_id = request.state.chute_id
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=f"No instances available (yet) for {chute_id=}",
        )

    # Initialize metrics.
    metrics = None
    if chute.standard_template == "vllm":
        if request.url.path.lstrip("/").startswith(("v1/chat", "v1/completion")):
            metrics = {
                "ttft": None,
                "tps": 0.0,
                "tokens": 0,
                "it": await count_prompt_tokens(request_body),
                "ot": 0,
            }
    elif chute.standard_template == "diffusion":
        steps = request_body.get("num_inference_steps", 25)
        if not isinstance(steps, int):
            try:
                steps = int(steps)
            except ValueError:
                steps = 25
        request_body["num_inference_steps"] = steps
        metrics = {
            "sps": 0,
            "steps": steps,
        }

    include_trace = request.headers.get("X-Chutes-Trace", "").lower() == "true"
    parent_invocation_id = str(uuid.uuid4())

    # Handle streaming responses, either because the user asked for X-Chutes-Trace,
    # or in the case of LLMs with stream: true in request.
    if stream or include_trace:
        # We have to wait until we have the first chuck to determine whether or not we
        # should return a successful response, otherwise for example we could return
        # a 200 status but actually be overwhelmed and it should be a 429/503/etc.
        async def _buffered_stream_response():
            first_chunk_processed = False
            buffered_chunks = []

            try:
                async for chunk in invoke(
                    chute,
                    current_user,
                    selected_cord["path"],
                    selected_cord["function"],
                    stream,
                    args,
                    kwargs,
                    manager,
                    parent_invocation_id,
                    metrics=metrics,
                    request=request,
                    prefixes=prefix_hashes,
                    raw_payload=raw_payload,
                ):
                    if include_trace:
                        if not first_chunk_processed:
                            first_chunk_processed = True
                        yield chunk
                        continue

                    # Handle errors.
                    if chunk.startswith('data: {"error"'):
                        chunk_data = json.loads(chunk[6:])
                        error = chunk_data["error"]

                        # If the error occurred on the first chunk, we can raise an HTTP exception.
                        if not first_chunk_processed:
                            # SGLang errors.
                            if isinstance(error, dict):
                                if (
                                    isinstance(error.get("code"), int)
                                    and 400 <= error["code"] < 500
                                ):
                                    logger.warning(
                                        f"Received error code from upstream streaming response: {error=}"
                                    )
                                    raise HTTPException(
                                        status_code=error["code"],
                                        detail=error,
                                    )

                            if error in ("infra_overload", "no_targets"):
                                raise HTTPException(
                                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                                    detail=chunk_data.get("detail") or error,
                                )
                            elif error == "bad_request":
                                raise HTTPException(
                                    status_code=status.HTTP_400_BAD_REQUEST,
                                    detail=chunk_data.get("detail") or error,
                                )
                            raise HTTPException(
                                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                                detail=error,
                            )
                        else:
                            # If we've already started streaming, at this point we can't change the response
                            # headers so we need to just include an error string in the stream.
                            yield json.dumps(
                                {
                                    "error": f"Unhandled exception during response stream: {error}",
                                }
                            )
                            return

                    # Normal result chunks.
                    elif chunk.startswith('data: {"result"'):
                        result_val = json.loads(chunk[6:])["result"]
                        if not first_chunk_processed:
                            first_chunk_processed = True
                            for buffered_chunk in buffered_chunks:
                                yield buffered_chunk
                            buffered_chunks = []
                        yield result_val

            except Exception as e:
                logger.error(f"Failed here: {e}\n{traceback.format_exc()}")
                if not first_chunk_processed:
                    if isinstance(e, HTTPException):
                        raise e
                    else:
                        raise HTTPException(
                            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=str(e)
                        )
                else:
                    yield json.dumps(
                        {
                            "error": f"Unhandled exception during response stream: {e}",
                        }
                    )

        # Create the response generator, but wait for the first chunk before returning
        # the StreamingResponse object so we don't incorrectly give a 200 response
        # for failed requests.
        try:
            generator = _buffered_stream_response()
            first_chunk = await generator.__anext__()

            async def _stream_with_first_chunk():
                yield first_chunk
                async for chunk in generator:
                    yield chunk

            return StreamingResponse(
                _stream_with_first_chunk(),
                media_type="text/event-stream",
                headers={"X-Chutes-InvocationID": parent_invocation_id},
            )

        except HTTPException:
            raise
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Unhandled error generating response: {e}",
            )

    # Non-streamed (which we actually do stream but we'll just return the first item)
    error = None
    response = None
    async for chunk in invoke(
        chute,
        current_user,
        selected_cord["path"],
        selected_cord["function"],
        stream,
        args,
        kwargs,
        manager,
        parent_invocation_id,
        metrics=metrics,
        request=request,
        prefixes=prefix_hashes,
        raw_payload=raw_payload,
    ):
        if response:
            continue
        if chunk.startswith('data: {"result"'):
            result = json.loads(chunk[6:])["result"]
            if "bytes" in result:
                raw_data = BytesIO(base64.b64decode(result["bytes"].encode()))

                async def _streamfile():
                    yield raw_data.getvalue()

                response = StreamingResponse(
                    _streamfile(),
                    media_type=result["content_type"],
                    headers={"X-Chutes-InvocationID": parent_invocation_id},
                )
            elif "text" in result:
                response = Response(
                    content=result["text"],
                    media_type=result["content_type"],
                    headers={"X-Chutes-InvocationID": parent_invocation_id},
                )
            else:
                response = Response(
                    content=json.dumps(result.get("json", result)).decode(),
                    media_type="application/json",
                    headers={
                        "Content-type": "application/json",
                        "X-Chutes-InvocationID": parent_invocation_id,
                    },
                )
        elif chunk.startswith('data: {"error"'):
            chunk_data = json.loads(chunk[6:])
            error = chunk_data["error"]
            if error in ("infra_overload", "no_targets"):
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=chunk_data.get("detail") or error,
                )
            elif error == "bad_request":
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=chunk_data.get("detail") or error,
                )
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=error,
            )

    if response:
        return response
    raise HTTPException(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        detail=error or "No result returned from upstream",
    )


@host_invocation_router.api_route(
    "{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "HEAD"]
)
async def hostname_invocation(
    request: Request,
    current_user: User = Depends(get_current_user(raise_not_found=False)),
    include_in_schema=False,
):
    request.state.started_at = time.time()

    # /v1/models endpoint for llm.chutes.ai is handled differently.
    if (
        request.state.chute_id == "__megallm__"
        and request.url.path == "/v1/models"
        and request.method.lower() == "get"
    ):
        return await get_llms()

    # The /v1/models endpoint can be checked with no auth, but otherwise we need users.
    if not current_user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Authentication required.",
        )

    # Mega LLM/diffusion request handler.
    if request.state.chute_id in ("__megallm__", "__megadiffuser__", "__megaembed__"):
        try:
            payload = await request.json()
        except Exception:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid request JSON: {str(exc)}",
            )

        # MistralAI gated this model for some reason.......
        model = payload.get("model", "")
        if model == "mistralai/Mistral-Small-3.1-24B-Instruct-2503":
            payload["model"] = "chutesai/Mistral-Small-3.1-24B-Instruct-2503"

        # THUDM -> zai-org namespace change.
        if model.startswith("THUDM/"):
            payload["model"] = re.sub(r"^THUDM/", "zai-org/", model)

        # Kimi K2 Instruct update to moonshotai/Kimi-K2-Instruct-0905
        if model in ("moonshotai/Kimi-K2-Instruct", "moonshotai/Kimi-K2-Instruct-75k"):
            payload["model"] = "moonshotai/Kimi-K2-Instruct-0905"

        # GLM-4.5V
        if model.lower() == "zai-org/glm-4.5v-fp8":
            payload["model"] = "zai-org/GLM-4.5V"

        # Hermes 4.
        elif model == "Zenith":
            payload["model"] = "NousResearch/Hermes-4-405B-FP8"
        elif model == "Meridian":
            payload["model"] = "NousResearch/Hermes-4-70B"
        elif model == "Proxima":
            payload["model"] = "NousResearch/Hermes-4-14B"

        # Migration of temp/test version of DeepSeek-R1 to "normal" one.
        if model == "deepseek-ai/DeepSeek-R1-sgtest":
            payload["model"] = "deepseek-ai/DeepSeek-R1"

        # TEE re-routes.
        if model == "deepseek-ai/DeepSeek-V3.2-Speciale":
            payload["model"] = "deepseek-ai/DeepSeek-V3.2-Speciale-TEE"
        elif model == "tngtech/TNG-R1T-Chimera":
            payload["model"] = "tngtech/TNG-R1T-Chimera-TEE"
        elif model == "zai-org/GLM-4.6":
            payload["model"] = "zai-org/GLM-4.6-TEE"
        elif model == "openai/gpt-oss-120b":
            payload["model"] = "openai/gpt-oss-120b-TEE"
        elif model == "deepseek-ai/DeepSeek-V3-0324":
            payload["model"] = "deepseek-ai/DeepSeek-V3-0324-TEE"
        elif model == "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8":
            payload["model"] = "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8-TEE"
        elif model == "deepseek-ai/DeepSeek-V3.2":
            payload["model"] = "deepseek-ai/DeepSeek-V3.2-TEE"
        elif model == "deepseek-ai/DeepSeek-V3.1":
            payload["model"] = "deepseek-ai/DeepSeek-V3.1-TEE"
        elif model == "deepseek-ai/DeepSeek-V3.1-Terminus":
            payload["model"] = "deepseek-ai/DeepSeek-V3.1-Terminus-TEE"
        elif model == "zai-org/GLM-4.7":
            payload["model"] = "zai-org/GLM-4.7-TEE"
        elif model == "zai-org/GLM-4.5":
            payload["model"] = "zai-org/GLM-4.5-TEE"
        elif model == "moonshotai/Kimi-K2-Thinking":
            payload["model"] = "moonshotai/Kimi-K2-Thinking-TEE"
        elif model == "deepseek-ai/DeepSeek-R1-0528":
            payload["model"] = "deepseek-ai/DeepSeek-R1-0528-TEE"
        elif model == "deepseek-ai/DeepSeek-R1":
            payload["model"] = "deepseek-ai/DeepSeek-R1-TEE"
        elif model == "Qwen/Qwen3-235B-A22B-Instruct-2507":
            payload["model"] = "Qwen/Qwen3-235B-A22B-Instruct-2507-TEE"
        elif model == "NousResearch/Hermes-4-405B-FP8":
            payload["model"] = "NousResearch/Hermes-4-405B-FP8-TEE"
        elif model == "Qwen/Qwen2.5-VL-72B-Instruct":
            payload["model"] = "Qwen/Qwen2.5-VL-72B-Instruct-TEE"
        elif model == "OpenGVLab/InternVL3-78B":
            payload["model"] = "OpenGVLab/InternVL3-78B-TEE"
        elif model == "mistralai/Devstral-2-123B-Instruct-2512":
            payload["model"] = "mistralai/Devstral-2-123B-Instruct-2512-TEE"

        # No file support currently.
        if isinstance(payload.get("messages"), list):
            for message in payload["messages"]:
                if isinstance(message, dict) and isinstance(message.get("content"), list):
                    for content_item in message["content"]:
                        if (
                            isinstance(content_item, dict)
                            and "file" in content_item
                            or "file_url" in content_item
                        ):
                            raise HTTPException(
                                status_code=status.HTTP_400_BAD_REQUEST,
                                detail="File content not currently supported",
                            )

        # Fix continue_final_message <=> add_generation_prompt incompatibility.
        if payload.get("continue_final_message") and payload.get("add_generation_prompt", True):
            messages = payload.get("messages", [])
            if isinstance(messages, list) and messages and messages[-1].get("role") == "assistant":
                payload["add_generation_prompt"] = False
            else:
                payload["continue_final_message"] = False
            logger.warning("Resolved continue_final_message/add_generation_prompt conflict")

        # Disable logprobs for all models, for now - 2026-01-29 JD
        if "affine" not in model.lower():
            payload.pop("logprobs", None)
            payload.pop("top_logprobs", None)

        # Header and/or model name options to enable thinking mode for various models.
        enable_thinking = False
        if (request.headers.get("X-Enable-Thinking") or "").lower() == "true":
            enable_thinking = True
        if model.endswith(":THINKING"):
            payload["model"] = payload["model"].split(":THINKING")[0]
            enable_thinking = True
        if enable_thinking:
            if "chat_template_kwargs" not in payload:
                payload["chat_template_kwargs"] = {}
            payload["chat_template_kwargs"].update(
                {
                    "thinking": True,
                    "enable_thinking": True,
                }
            )

        # Normalize the chat template kwargs thinking keys since there are two...
        if "chat_template_kwargs" in payload:
            if (
                "thinking" in payload["chat_template_kwargs"]
                and "enable_thinking" not in payload["chat_template_kwargs"]
            ):
                payload["chat_template_kwargs"]["enable_thinking"] = payload[
                    "chat_template_kwargs"
                ]["thinking"]
            if (
                "enable_thinking" in payload["chat_template_kwargs"]
                and "thinking" not in payload["chat_template_kwargs"]
            ):
                payload["chat_template_kwargs"]["thinking"] = payload["chat_template_kwargs"][
                    "enable_thinking"
                ]
            if "thinking" not in payload["chat_template_kwargs"] and model.startswith(
                (
                    "deepseek-ai/DeepSeek-V3.2-Speciale",
                    "zai-org/GLM-4.7",
                    "moonshotai/Kimi-K2.5",
                )
            ):
                payload["chat_template_kwargs"]["thinking"] = True
                payload["chat_template_kwargs"]["enable_thinking"] = True
        elif model in (
            "deepseek-ai/DeepSeek-V3.2-Speciale",
            "deepseek-ai/DeepSeek-V3.2-Speciale-TEE",
            "zai-org/GLM-4.7",
            "zai-org/GLM-4.7-TEE",
            "moonshotai/Kimi-K2.5",
            "moonshotai/Kimi-K2.5-TEE",
        ):
            payload["chat_template_kwargs"] = {"thinking": True, "enable_thinking": True}

        # Auto tool choice default.
        if payload.get("tools") and "tool_choice" not in payload:
            payload["tool_choice"] = "auto"

        model = payload.get("model")
        chute = None
        template = (
            "vllm"
            if "llm" in request.state.chute_id
            else "embedding"
            if "embed" in request.state.chute_id
            else "diffusion"
        )
        if model:
            if (chute := await get_one(model)) is None:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"model not found: {model}",
                )
            if chute.standard_template != template or (
                not chute.public
                and (
                    chute.user_id != current_user.user_id
                    and not await is_shared(chute.chute_id, current_user.user_id)
                )
                and not subnet_role_accessible(chute, current_user)
            ):
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"model not found: {model}",
                )
            request.state.chute_id = chute.chute_id
            request.state.auth_object_id = chute.chute_id

    # # Model disabled temporarily?
    # if (
    #     await settings.redis_client.get(f"model_disabled:{request.state.chute_id}")
    #     and current_user.user_id != "dff3e6bb-3a6b-5a2b-9c48-da3abcd5ca5f"
    # ):
    #     logger.warning(f"MODEL DISABLED: {request.state.chute_id}")
    #     raise HTTPException(
    #         status_code=status.HTTP_429_TOO_MANY_REQUESTS,
    #         detail="model is under maintenance",
    #     )

    return await _invoke(request, current_user)
